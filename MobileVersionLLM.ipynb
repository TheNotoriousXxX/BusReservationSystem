{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheNotoriousXxX/BusReservationSystem/blob/terminator2.0/MobileVersionLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm85Ap3zDmYB"
      },
      "source": [
        "# Getting Started with MLC-LLM using the Llama 2 Model\n",
        "\n",
        "Here's a quick overview of how to get started with the MLC-LLM `ChatModule` in Python. In this tutorial, we will chat with the [Llama2](https://ai.meta.com/llama/) model. For the easiest setup, we recommend trying this out in a Google Colab notebook. Click the button below to get started!\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ttPt-hNDmYC"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Let's set up your environment, so you can successfully run the `ChatModule`. First, let's set up the Conda environment which we will be running this notebook in (not required if running in Google Colab).\n",
        "\n",
        "```bash\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```\n",
        "\n",
        "**Google Colab:** If you are running this in a Google Colab notebook, be sure to change your runtime to GPU by going to Runtime > Change runtime type and setting the Hardware accelerator to be \"GPU\". Select \"Connect\" on the top right to instantiate your GPU session.\n",
        "\n",
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the version number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KK25HZsIDmYC",
        "outputId": "f5b5b42a-d4b4-45d4-bcdf-236693090095",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug 17 19:36:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWOtpjJMDmYE"
      },
      "source": [
        "Next, let's download the MLC-AI and MLC-Chat nightly build packages. Go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PgW-5OAADmYE",
        "outputId": "4bba36ca-748d-4368-8b4b-e1ead6b82e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu118-0.12.dev1398-cp310-cp310-manylinux_2_28_x86_64.whl (97.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlc-chat-nightly-cu118\n",
            "  Using cached https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu118-0.1.dev373-cp310-cp310-manylinux_2_28_x86_64.whl (20.7 MB)\n",
            "Collecting attrs (from mlc-ai-nightly-cu118)\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting cloudpickle (from mlc-ai-nightly-cu118)\n",
            "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting decorator (from mlc-ai-nightly-cu118)\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting ml-dtypes (from mlc-ai-nightly-cu118)\n",
            "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Collecting numpy (from mlc-ai-nightly-cu118)\n",
            "  Using cached numpy-1.26.0b1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
            "Collecting psutil (from mlc-ai-nightly-cu118)\n",
            "  Using cached psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "Collecting scipy (from mlc-ai-nightly-cu118)\n",
            "  Using cached scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "Collecting tornado (from mlc-ai-nightly-cu118)\n",
            "  Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "Collecting typing-extensions (from mlc-ai-nightly-cu118)\n",
            "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting fastapi (from mlc-chat-nightly-cu118)\n",
            "  Using cached fastapi-0.101.1-py3-none-any.whl (65 kB)\n",
            "Collecting uvicorn (from mlc-chat-nightly-cu118)\n",
            "  Using cached uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "Collecting shortuuid (from mlc-chat-nightly-cu118)\n",
            "  Using cached shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached pydantic-2.2.0-py3-none-any.whl (373 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "Collecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Collecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
            "Collecting pydantic-core==2.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached pydantic_core-2.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "Collecting anyio<5,>=3.4.0 (from starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached anyio-4.0.0rc1-py3-none-any.whl (82 kB)\n",
            "Collecting idna>=2.8 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu118, fastapi, mlc-chat-nightly-cu118\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.3.3\n",
            "    Uninstalling tornado-6.3.3:\n",
            "      Successfully uninstalled tornado-6.3.3\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.0\n",
            "    Uninstalling sniffio-1.3.0:\n",
            "      Successfully uninstalled sniffio-1.3.0\n",
            "  Attempting uninstall: shortuuid\n",
            "    Found existing installation: shortuuid 1.0.11\n",
            "    Uninstalling shortuuid-1.0.11:\n",
            "      Successfully uninstalled shortuuid-1.0.11\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.0b1\n",
            "    Uninstalling numpy-1.26.0b1:\n",
            "      Successfully uninstalled numpy-1.26.0b1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.1.3\n",
            "    Uninstalling exceptiongroup-1.1.3:\n",
            "      Successfully uninstalled exceptiongroup-1.1.3\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.1.1\n",
            "    Uninstalling decorator-5.1.1:\n",
            "      Successfully uninstalled decorator-5.1.1\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.5.0\n",
            "    Uninstalling annotated-types-0.5.0:\n",
            "      Successfully uninstalled annotated-types-0.5.0\n",
            "  Attempting uninstall: uvicorn\n",
            "    Found existing installation: uvicorn 0.23.2\n",
            "    Uninstalling uvicorn-0.23.2:\n",
            "      Successfully uninstalled uvicorn-0.23.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.1\n",
            "    Uninstalling scipy-1.11.1:\n",
            "      Successfully uninstalled scipy-1.11.1\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.6.0\n",
            "    Uninstalling pydantic_core-2.6.0:\n",
            "      Successfully uninstalled pydantic_core-2.6.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.0.0rc1\n",
            "    Uninstalling anyio-4.0.0rc1:\n",
            "      Successfully uninstalled anyio-4.0.0rc1\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.27.0\n",
            "    Uninstalling starlette-0.27.0:\n",
            "      Successfully uninstalled starlette-0.27.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.2.0\n",
            "    Uninstalling pydantic-2.2.0:\n",
            "      Successfully uninstalled pydantic-2.2.0\n",
            "  Attempting uninstall: mlc-ai-nightly-cu118\n",
            "    Found existing installation: mlc-ai-nightly-cu118 0.12.dev1398\n",
            "    Uninstalling mlc-ai-nightly-cu118-0.12.dev1398:\n",
            "      Successfully uninstalled mlc-ai-nightly-cu118-0.12.dev1398\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.101.1\n",
            "    Uninstalling fastapi-0.101.1:\n",
            "      Successfully uninstalled fastapi-0.101.1\n",
            "  Attempting uninstall: mlc-chat-nightly-cu118\n",
            "    Found existing installation: mlc-chat-nightly-cu118 0.1.dev373\n",
            "    Uninstalling mlc-chat-nightly-cu118-0.1.dev373:\n",
            "      Successfully uninstalled mlc-chat-nightly-cu118-0.1.dev373\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.0b1 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado==6.3.2, but you have tornado 6.3.3 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.0.0rc1 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0b1 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.0b1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.5.0 anyio-4.0.0rc1 attrs-23.1.0 click-8.1.7 cloudpickle-2.2.1 decorator-5.1.1 exceptiongroup-1.1.3 fastapi-0.101.1 h11-0.14.0 idna-3.4 ml-dtypes-0.2.0 mlc-ai-nightly-cu118-0.12.dev1398 mlc-chat-nightly-cu118-0.1.dev373 numpy-1.26.0b1 psutil-5.9.5 pydantic-2.2.0 pydantic-core-2.6.0 scipy-1.11.1 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 tornado-6.3.3 typing-extensions-4.7.1 uvicorn-0.23.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator",
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "ohpPM7vtADU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn7MYEFt5tvY"
      },
      "source": [
        "**Google Colab:** If in Google Colab, you may see a message warning you to restart the runtime. Simply run the following code in a new code cell to restart the runtime.\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwsWd1WbDmYE"
      },
      "source": [
        "Next, let's download the model weights for the Llama2 model and the prebuilt model libraries from Github. In order to download the large weights, we'll have to use `git lfs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppvAhErV3gjq"
      },
      "source": [
        "Note: If you are NOT running in **Google Colab** you may need to run this line `!conda install git git-lfs` to install `git` and `git-lfs` before running the following cell to fully install `git lfs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V0GjINnMDmYF",
        "outputId": "8d1ef184-5046-4f86-d2e2-5dbbed6eed69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYwjsCOK7Jij"
      },
      "source": [
        "These commands will download many prebuilt libraries as well as the chat configuration for Llama-2-7b that `mlc_chat` needs, which may take a long time. If in **Google Colab** you can verify that the files are being downloaded by clicking on the folder icon on the left and navigating to the `dist` and then `prebuilt` folders which should be updating as the files are being downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FSAe7Ew_DmYF",
        "outputId": "595944cf-2f4a-404b-aefb-947fb549be08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dist/prebuilt/lib'...\n",
            "remote: Enumerating objects: 247, done.\u001b[K\n",
            "remote: Counting objects: 100% (112/112), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 247 (delta 82), reused 76 (delta 51), pack-reused 135\u001b[K\n",
            "Receiving objects: 100% (247/247), 58.23 MiB | 14.65 MiB/s, done.\n",
            "Resolving deltas: 100% (175/175), done.\n",
            "Updating files: 100% (75/75), done.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p dist/prebuilt\n",
        "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BDbi6H3MDmYF",
        "outputId": "5e2b59be-f1d2-4ca7-af8f-0350949e7915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlc-chat-Llama-2-7b-chat-hf-q4f16_1'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects:  33% (1/3)\u001b[K\rremote: Counting objects:  66% (2/3)\u001b[K\rremote: Counting objects: 100% (3/3)\u001b[K\rremote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 126 (delta 0), reused 3 (delta 0), pack-reused 123\u001b[K\n",
            "Receiving objects: 100% (126/126), 497.08 KiB | 3.71 MiB/s, done.\n",
            "Filtering content: 100% (116/116), 3.53 GiB | 77.44 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ru5__tDmYF"
      },
      "source": [
        "## Let's Chat!\n",
        "\n",
        "Before we can chat with the model, we must first import a library and instantiate a `ChatModule` instance. The `ChatModule` must be initialized with the appropriate model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AJAt6oW7DmYF",
        "outputId": "c1c70524-29da-4b43-8135-95c503ed8e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System automatically detected device: cuda\n",
            "Using model folder: /content/dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\n",
            "Using mlc chat config: /content/dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json\n",
            "Using library model: /content/dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from mlc_chat import ChatModule\n",
        "from mlc_chat.callback import StreamToStdout\n",
        "\n",
        "cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9m5sxyXDmYF"
      },
      "source": [
        "Note that the above invocation abstracts away the logic for finding the relevant model directory and prebuilt library paths. To specify these manually, you could run the following instead (which would be equivalent to the above).\n",
        "\n",
        "```python\n",
        "cm = ChatModule(model=\"dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\", lib_path=\"dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEaVXnnJDmYF"
      },
      "source": [
        "That is all what needed to set up the `ChatModule`. You can now chat with the model by entering any prompt you'd like. Try it out below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TNmg9N_NDmYF",
        "outputId": "6ea9eb9e-9396-4414-8846-ff4e96c6c57a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! Here is a simple demo Python code that prints \"Hello, World!\" to the console:\n",
            "```\n",
            "print(\"Hello, World!\")\n",
            "```\n",
            "This code uses the `print()` function to display the string \"Hello, World!\" on the console.\n",
            "\n",
            "Would you like to see more code examples? Let me know and I'd be happy to help!\n"
          ]
        }
      ],
      "source": [
        "output = cm.generate(\n",
        "    prompt=\"give me a demo python code\",\n",
        "    progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSWUVjXp_eQW"
      },
      "source": [
        "You can also repeat running the code block below for multiple rounds to interact with the model in a chat style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN812s62_eQW"
      },
      "outputs": [],
      "source": [
        "prompt = input(\"Prompt: \")\n",
        "output = cm.generate(prompt=prompt, progress_callback=StreamToStdout(callback_interval=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-BSn5dx_eQX"
      },
      "outputs": [],
      "source": [
        "output = cm.generate(\n",
        "    prompt=\"Please summarize your response in three sentences.\",\n",
        "    progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4bOyUk7DmYF"
      },
      "source": [
        "To check the generation speed of the chat bot, you can print the statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPbPj6vpDmYF"
      },
      "outputs": [],
      "source": [
        "print(cm.stats())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAb-XZPnDmYF"
      },
      "source": [
        "By default, the `ChatModule` will keep a history of your chat. You can reset the chat history by running the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKpKgVxNDmYF"
      },
      "outputs": [],
      "source": [
        "cm.reset_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5sKqKYQ_eQY"
      },
      "source": [
        "### Benchmark Performance\n",
        "\n",
        "To benchmark the performance, we can use the `benchmark_generate` method of ChatModule. It takes an input prompt and the number of tokens to generate, ignores the system prompt and model stop criterion, generates tokens in a language model way and stops until finishing generating the desired number of tokens. After calling `benchmark_generate`, we can use `stats` to check the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GBf31HH_eQY"
      },
      "outputs": [],
      "source": [
        "print(cm.benchmark_generate(prompt=\"What is benchmark?\", generate_length=512))\n",
        "cm.stats()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}